{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d50466",
   "metadata": {
    "id": "43d50466"
   },
   "source": [
    "## 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40473d18",
   "metadata": {
    "id": "40473d18"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from xlwt import *\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from googletrans import Translator\n",
    "from textblob import TextBlob\n",
    "from sklearn import feature_extraction, metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enchant\n",
    "import textwrap\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0eac3",
   "metadata": {
    "id": "e6a0eac3"
   },
   "source": [
    "## 2. Build the Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740808a",
   "metadata": {
    "id": "1740808a"
   },
   "outputs": [],
   "source": [
    "def web_scraper(in_url, tag, html_class, target_tag):\n",
    "\n",
    "    f = requests.get(in_url)\n",
    "    soup = BeautifulSoup(f.content,'lxml')\n",
    "    \n",
    "    content=soup.find(tag,{'class':html_class}).find_all(target_tag)\n",
    "    \n",
    "    return (content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24c0ce",
   "metadata": {
    "id": "ee24c0ce"
   },
   "source": [
    "## 3. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ca7a1",
   "metadata": {
    "id": "186ca7a1"
   },
   "source": [
    "#### First website\n",
    "#### URL: https://ling-app.com/ms/malay-slang-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9c69d",
   "metadata": {
    "id": "27a9c69d",
    "outputId": "0d4b682a-2dbb-493e-f771-6608eda73048",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for content (url 1):  32\n",
      "Length for meaning (url 1):  32\n",
      "\n",
      "Abbreviation:\n",
      "Length for content (url 1):  18\n",
      "Length for meaning (url 1):  18\n"
     ]
    }
   ],
   "source": [
    "url_1_c = []\n",
    "url_1_m = []\n",
    "\n",
    "url_1_c_s = []\n",
    "url_1_m_s = []\n",
    "\n",
    "#Access URL and crawl the content\n",
    "url_1 = \"https://ling-app.com/ms/malay-slang-words/\"\n",
    "\n",
    "web_content_1 = web_scraper(url_1,'div','ct-inner-content','h3')\n",
    "\n",
    "web_content_2 = web_scraper(url_1,'div','ct-inner-content','p')\n",
    "\n",
    "#Store the content into an array\n",
    "i = 0;\n",
    "for x in web_content_1:\n",
    "    i = i+1\n",
    "    if i in range(1,10):\n",
    "        url_1_c.append(x.text.strip()[3:])\n",
    "    else:\n",
    "        url_1_c.append(x.text.strip()[4:])\n",
    "    \n",
    "    \n",
    "        \n",
    "#Store the meaning of the content accordingly into an array\n",
    "i = 0;\n",
    "for x in web_content_2:\n",
    "    i = i+1\n",
    "    if i > 7:\n",
    "        url_1_m.append((x.text.strip())) \n",
    "        if i == 69 or i == 73:\n",
    "            url_1_m[i-9] = url_1_m[i-9]+' '+url_1_m[i-8]\n",
    "            \n",
    "remove_= [65,61,51,50,49]\n",
    "for i in remove_:\n",
    "    del url_1_m[i]\n",
    "    \n",
    "url_1_m = url_1_m[::3]\n",
    "url_1_m.pop()\n",
    "\n",
    "\n",
    "#Second attempt\n",
    "web_content_3 = web_scraper(url_1, 'figure','wp-block-table','td')\n",
    "\n",
    "#Store the content and meaning into one array\n",
    "c_m = []\n",
    "c_m2 = []\n",
    "\n",
    "for x in web_content_3:\n",
    "    c_m.append(x.text.strip())\n",
    "del c_m[:4]\n",
    "\n",
    "N=4\n",
    "c_m2 = [c_m[n:n+N] for n in range(0, len(c_m), 4)]\n",
    "\n",
    "for x in c_m2:\n",
    "    url_1_c_s.append(x[0])\n",
    "    if x[3] == '-':\n",
    "        url_1_m_s.append(x[2])\n",
    "    else:\n",
    "        url_1_m_s.append(x[3])\n",
    "\n",
    "print('Length for content (url 1): ', (len(url_1_c)))\n",
    "print('Length for meaning (url 1): ', (len(url_1_m)))\n",
    "\n",
    "print('\\nAbbreviation:')\n",
    "print('Length for content (url 1): ', (len(url_1_c_s)))\n",
    "print('Length for meaning (url 1): ', (len(url_1_m_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37125a0",
   "metadata": {
    "id": "c37125a0"
   },
   "source": [
    "#### Second website\n",
    "#### URL: https://www.shopback.my/blog/bahasa-rojak-malaysian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1eb14",
   "metadata": {
    "id": "cff1eb14",
    "outputId": "7828e71b-c881-4a73-ca86-8b440d4c6fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for content (url 2):  64\n",
      "Length for meaning (url 2):  64\n"
     ]
    }
   ],
   "source": [
    "url_2_c = []\n",
    "url_2_m = []\n",
    "\n",
    "#Access URL and crawl the content\n",
    "url_2 = \"https://www.shopback.my/blog/bahasa-rojak-malaysian\"\n",
    "\n",
    "web_content_1 = web_scraper(url_2,'div','post-content','h2')\n",
    "\n",
    "web_content_2 = web_scraper(url_2,'div','post-content','p')\n",
    "\n",
    "#Store the content and meaning into one array\n",
    "i = 0;\n",
    "for x in web_content_1:\n",
    "    i = i+1\n",
    "    if i in range(1,10):\n",
    "        str1=x.text.strip()[3:-1]\n",
    "\n",
    "    else:\n",
    "        str1=x.text.strip()[4:-1]\n",
    "\n",
    "    if str1.find(\"/\") == -1:\n",
    "        url_2_c.append(str1)\n",
    "                 \n",
    "    else:\n",
    "        url_2_c.append(str1[:str1.find(\"/\")].strip())\n",
    "        url_2_c.append(str1[str1.find(\"/\")+1:].strip())\n",
    "\n",
    "        \n",
    "i = 0\n",
    "for x in web_content_2:\n",
    "    i = i+1\n",
    "    if str(x.text.strip())!= \"\":\n",
    "        if i == 44 or i == 46 or i == 58:\n",
    "            url_2_m.append(x.text.strip())\n",
    "        url_2_m.append(x.text.strip())\n",
    "    \n",
    "del url_2_m[-3:]\n",
    "del url_2_m[:3]\n",
    "\n",
    "\n",
    "print('Length for content (url 2): ', (len(url_2_c)))\n",
    "print('Length for meaning (url 2): ', (len(url_2_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a6611",
   "metadata": {
    "id": "223a6611"
   },
   "source": [
    "#### Third website\n",
    "#### URL: https://selongkar10.blogspot.com/2019/04/60-singkatan-perkataan-melayu-dalam.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e61b77",
   "metadata": {
    "id": "f1e61b77",
    "outputId": "1b893c4d-926a-49d5-8eda-4ae495205fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation:\n",
      "Length for content (url 3):  58\n",
      "Length for meaning (url 3):  58\n"
     ]
    }
   ],
   "source": [
    "c_m = []\n",
    "url_3_c_s = []\n",
    "url_3_m_s = []\n",
    "\n",
    "#Access URL and crawl the content\n",
    "url_3 = \"https://selongkar10.blogspot.com/2019/04/60-singkatan-perkataan-melayu-dalam.html\"\n",
    "\n",
    "web_content_1 = web_scraper(url_3,'div','post-body post-content','div')\n",
    "\n",
    "#Store the content and meaning into one array \n",
    "for x in web_content_1:\n",
    "    if str(x.text.strip()) != \"\":\n",
    "        c_m.append(x.text.strip())\n",
    "\n",
    "del c_m[:2]\n",
    "del c_m[-15:]\n",
    "del c_m[47]\n",
    "\n",
    "c_m2 = []\n",
    "i = 0\n",
    "for x in c_m:\n",
    "    i = i+1\n",
    "    if i != 22:\n",
    "        x = x.replace('..',' :')\n",
    "        x = x.replace('-',':')\n",
    "        x = x.replace('=',':')\n",
    "        x = x.replace('\\n',\"\")\n",
    "        c_m2.append(str(x).strip())\n",
    "\n",
    "\n",
    "c_m2.append(c_m2[32][c_m2[32].find(\":M\")+1:])\n",
    "c_m2[32] = c_m2[32][:c_m2[32].find(\":M\")]\n",
    "c_m2[10]=c_m2[10][7:]\n",
    "\n",
    "for x in c_m2:\n",
    "    url_3_c_s.append(x[:x.find(\" : \")].strip())\n",
    "    url_3_m_s.append(x[x.find(\" : \")+3:].strip())\n",
    "\n",
    "print('Abbreviation:')\n",
    "print('Length for content (url 3): ', (len(url_3_c_s)))\n",
    "print('Length for meaning (url 3): ', (len(url_3_m_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23475b4",
   "metadata": {
    "id": "a23475b4"
   },
   "source": [
    "#### Fourth website\n",
    "#### URL: https://www.quora.com/What-are-the-common-colloquial-bahasa-melayu-short-forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcb1a0",
   "metadata": {
    "id": "e6dcb1a0"
   },
   "source": [
    "##### Since the input added to this web page by Javascript, so the html does not contain any elements, we have to use Selenium web driver instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc7da8",
   "metadata": {
    "id": "a0fc7da8",
    "outputId": "61cee166-0e2d-4d63-feb3-e44196b0df66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\septe\\AppData\\Local\\Temp/ipykernel_10132/1684031575.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Firefox(executable_path='C:/ProgramData/geckodriver.exe')\n",
      "C:\\Users\\septe\\AppData\\Local\\Temp/ipykernel_10132/1684031575.py:14: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  spans = driver.find_elements_by_css_selector('span.q-box.qu-userSelect--text')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation:\n",
      "Length for content (url 4):  147\n",
      "Length for meaning (url 4):  147\n"
     ]
    }
   ],
   "source": [
    "c_m = []\n",
    "\n",
    "url_4_c_s = []\n",
    "url_4_m_s = []\n",
    "\n",
    "#Have to download geckodriver and place under this path\n",
    "#link: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "driver = webdriver.Firefox(executable_path='C:/ProgramData/geckodriver.exe')\n",
    "\n",
    "url_4 = \"https://www.quora.com/What-are-the-common-colloquial-bahasa-melayu-short-forms\"\n",
    "driver.get(url_4)\n",
    "\n",
    "spans = driver.find_elements_by_css_selector('span.q-box.qu-userSelect--text')\n",
    "\n",
    "c_m = spans[1].text.split('\\n')\n",
    "\n",
    "del c_m[:1]\n",
    "del c_m[-1:]\n",
    "\n",
    "c_m[95] = c_m[95].replace('/',\" \")\n",
    "\n",
    "for x in c_m:\n",
    "    if x.find(\"—\") != -1:\n",
    "        if x.find(\"/\") != -1:\n",
    "            s1=re.search('/(.*) —', x).group(1)\n",
    "            s2=re.search('(.*)/', x).group(1)\n",
    "            url_4_c_s.append(s1.strip())\n",
    "            url_4_c_s.append(s2.strip())\n",
    "            url_4_m_s.append(x[x.find(\"—\")+2:].strip())\n",
    "            url_4_m_s.append(x[x.find(\"—\")+2:].strip())\n",
    "        else:\n",
    "            url_4_c_s.append(x[:x.find(\"—\")].strip())\n",
    "            url_4_m_s.append(x[x.find(\"—\")+2:].strip())\n",
    "\n",
    "print('Abbreviation:')\n",
    "print('Length for content (url 4): ', (len(url_4_c_s)))\n",
    "print('Length for meaning (url 4): ', (len(url_4_m_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebe006",
   "metadata": {
    "id": "f7ebe006"
   },
   "source": [
    "#### Fifth website\n",
    "#### URL: https://cilisos.my/bahasa-sms-shortforms-glossary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59d72b",
   "metadata": {
    "id": "1d59d72b",
    "outputId": "13350381-1c13-4a52-bf7e-acf21acff7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation:\n",
      "Length for content (url 5):  593\n",
      "Length for meaning (url 5):  593\n"
     ]
    }
   ],
   "source": [
    "c_m = []\n",
    "url_5_c_s = []\n",
    "url_5_m_s = []\n",
    "\n",
    "#Access URL and crawl the content\n",
    "url_5 = \"https://cilisos.my/bahasa-sms-shortforms-glossary/\"\n",
    "\n",
    "web_content_1 = web_scraper(url_5,'div','post-cont-in','li')\n",
    "\n",
    "#Store the content and meaning into one array\n",
    "for x in web_content_1:\n",
    "    if str(x).find(\"[word]\") == -1:\n",
    "        c_m.append(x.text.strip())\n",
    "    \n",
    "del c_m[-7:]\n",
    "del c_m[:8]\n",
    "\n",
    "for x in c_m:\n",
    "    if str(x).find(':') != -1:\n",
    "        x = x.split(':')\n",
    "        n = x[0].count(',')\n",
    "\n",
    "        for i in range(1,n+2):\n",
    "            url_5_m_s.append(x[-1].strip())\n",
    "            \n",
    "        for x in x[0].split(','):\n",
    "            url_5_c_s.append(x.strip())\n",
    "\n",
    "print('Abbreviation:')\n",
    "print('Length for content (url 5): ', (len(url_5_c_s)))\n",
    "print('Length for meaning (url 5): ', (len(url_5_m_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c4584",
   "metadata": {
    "id": "ae9c4584"
   },
   "source": [
    "#### Sixth website\n",
    "#### URL: https://ai.glossika.com/blog/an-introduction-to-manglish-malaysias-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f24b55",
   "metadata": {
    "id": "a2f24b55",
    "outputId": "d998533b-237a-41ec-ca38-c6cb6c7d8068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for content (url 6):  59\n",
      "Length for meaning (url 6):  59\n"
     ]
    }
   ],
   "source": [
    "c_m = []\n",
    "\n",
    "url_6_c = []\n",
    "url_6_m = []\n",
    "\n",
    "#Access URL and crawl the content\n",
    "url_6 = \"https://ai.glossika.com/blog/an-introduction-to-manglish-malaysias-english\"\n",
    "\n",
    "web_content_1 = web_scraper(url_6,'div','post-content','td')\n",
    "\n",
    "#Store the content and meaning into one array\n",
    "for x in web_content_1:\n",
    "    if x.text.strip() != \"\":\n",
    "        c_m.append(x.text.strip())\n",
    "        \n",
    "del c_m[184:]\n",
    "del c_m[102]\n",
    "del c_m[42:51]\n",
    "\n",
    "for i in range(0,6):\n",
    "    if (i % 2) == 0:\n",
    "        url_6_c.append(c_m[i])\n",
    "        url_6_m.append(c_m[i+1])\n",
    "\n",
    "del c_m[0:6]\n",
    "\n",
    "combined_content=[]\n",
    "new_content=[]\n",
    "idx_1=0\n",
    "idx_2=3\n",
    "\n",
    "while idx_2<=len(c_m):\n",
    "    combined_content=[]\n",
    "    for i in range(idx_1,idx_2):\n",
    "        combined_content.append(c_m[i])\n",
    "    new_content.append(combined_content)\n",
    "    idx_1 += 3\n",
    "    idx_2 += 3\n",
    "    \n",
    "for x in new_content:\n",
    "    url_6_c.append(x[0].strip())\n",
    "    url_6_m.append(x[2].strip())\n",
    "    \n",
    "print('Length for content (url 6): ', (len(url_6_c)))\n",
    "print('Length for meaning (url 6): ', (len(url_6_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6db76",
   "metadata": {
    "id": "7ea6db76"
   },
   "source": [
    "## 4. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba676dc6",
   "metadata": {
    "id": "ba676dc6"
   },
   "outputs": [],
   "source": [
    "def textFilter(x, is_trans_lemma): # is_trans_lemma = '0' (X perform translation & lemmatization) / '1' (Does perform)\n",
    "    \n",
    "    #translate the phrases\n",
    "    if is_trans_lemma == 1:\n",
    "        translator = Translator()\n",
    "        x = translator.translate(x)\n",
    "        \n",
    "    #remove phrases that start with @, #, http\n",
    "    if is_trans_lemma == 1:\n",
    "        token = x.text.split()\n",
    "    elif is_trans_lemma == 0:\n",
    "        token = x.split()\n",
    "    out = []\n",
    "    stopphrases = ['@','RT','http', '#']\n",
    "    for i in token:\n",
    "        i = str(i)\n",
    "        for stopphrase in stopphrases:\n",
    "            if stopphrase in i:\n",
    "                out.append(i)\n",
    "                break\n",
    "    token  = [item.lower() for item in token if item not in out]\n",
    "    z = \" \".join(token)\n",
    "    \n",
    "    #remove punctuation from indivual words and remove non alphanumeric words\n",
    "    token = word_tokenize(z)\n",
    "    if is_trans_lemma == 1:\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "    elif is_trans_lemma == 0:\n",
    "        stripped = token\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #lemmatization\n",
    "    if is_trans_lemma == 1:\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per = percentage of text remain after summarized (0-1)\n",
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1f22f",
   "metadata": {
    "id": "20f1f22f"
   },
   "source": [
    "## 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2ff886",
   "metadata": {
    "id": "ef2ff886"
   },
   "outputs": [],
   "source": [
    "def sentiment_polarity(x):\n",
    "    blob = TextBlob(str(x))\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "\n",
    "def sentiment_label(x):\n",
    "    if x > 0.7:\n",
    "        label='strong positive'\n",
    "    elif x < -0.7:\n",
    "        label='strong negative'\n",
    "    elif x == 0:\n",
    "        label='neutral'\n",
    "    elif x > 0:\n",
    "        label='positive'\n",
    "    else:\n",
    "        label='negative'\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36962970",
   "metadata": {
    "id": "36962970"
   },
   "source": [
    "## 6. Combine all the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c2eaf",
   "metadata": {
    "id": "cf9c2eaf"
   },
   "outputs": [],
   "source": [
    "c = [url_1_c, url_2_c, url_6_c, url_1_c_s, url_3_c_s, url_4_c_s, url_5_c_s]\n",
    "m = [url_1_m, url_2_m, url_6_m, url_1_m_s, url_3_m_s, url_4_m_s, url_5_m_s]\n",
    "\n",
    "cmplt_content = []\n",
    "cmplt_meaning = []\n",
    "sentiment_score = []\n",
    "\n",
    "line=1\n",
    "\n",
    "for x in c:\n",
    "    cmplt_content.append(x)\n",
    "    \n",
    "flattened_cmplt_content = [val for sublist in cmplt_content for val in sublist]\n",
    "\n",
    "for x in m:\n",
    "    cmplt_meaning.append(x)\n",
    "\n",
    "flattened_cmplt_meaning = [val for sublist in cmplt_meaning for val in sublist]\n",
    "    \n",
    "for i in range(0, len(flattened_cmplt_meaning)):\n",
    "    sentiment_score.append(sentiment_polarity(summarize(textFilter(flattened_cmplt_meaning[i], 1), 0.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c09028",
   "metadata": {
    "id": "a4c09028"
   },
   "source": [
    "## 7. Similarity check in order to remove high similarity words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee00fd",
   "metadata": {
    "id": "28ee00fd"
   },
   "outputs": [],
   "source": [
    "def similarity_check(main_content):\n",
    "    \n",
    "    threshold = 0.8\n",
    "    removed_idxs = []\n",
    "    \n",
    "    for x in main_content:\n",
    "       \n",
    "        vectorizer = feature_extraction.text.CountVectorizer()   \n",
    "        X = vectorizer.fit_transform([x]+main_content).toarray()\n",
    "        lst_vectors = [vec for vec in X]\n",
    "        cosine_sim = metrics.pairwise.cosine_similarity(lst_vectors)\n",
    "        scores = cosine_sim[0][1:]\n",
    "        \n",
    "        match_scores = scores[scores >= threshold]\n",
    "        \n",
    "            \n",
    "        if len(match_scores) >1:\n",
    "            match_idxs = [i for i in np.where(scores >= threshold)[0]]\n",
    "            match_strings = [main_content[i] for i in match_idxs]\n",
    "            match_idxs.sort(reverse=True)\n",
    "            \n",
    "            #solution\n",
    "            \n",
    "            #1) Store their respectively sentiment score based on the index\n",
    "            s_score=[]\n",
    "            for i in match_idxs:\n",
    "                s_score.append(sentiment_score[i])\n",
    "            \n",
    "                \n",
    "            #2) Check if the polarity of the average sentiment score\n",
    "            if (sum(s_score)/len(s_score)) >0:\n",
    "                desired_score_idx = s_score.index(max(s_score))\n",
    "                    \n",
    "            elif (sum(s_score)/len(s_score)) <0:\n",
    "                desired_score_idx = s_score.index(min(s_score))\n",
    "                    \n",
    "                \n",
    "            #3) Store the weaker polarity indexes into a list\n",
    "            for i in match_idxs:\n",
    "                if match_idxs.index(i) != desired_score_idx:\n",
    "                    if i not in removed_idxs:\n",
    "                        removed_idxs.append(i)\n",
    "\n",
    "\n",
    "    return (removed_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426fe1c",
   "metadata": {
    "id": "7426fe1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_idxs = similarity_check(flattened_cmplt_content)\n",
    "removed_idxs.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMZT3xzEqSHA",
   "metadata": {
    "id": "fMZT3xzEqSHA"
   },
   "outputs": [],
   "source": [
    "#delete all the indexes with high similarity\n",
    "for i in removed_idxs:\n",
    "    del flattened_cmplt_content[i]\n",
    "    del flattened_cmplt_meaning[i]\n",
    "    del sentiment_score[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16945e89",
   "metadata": {
    "id": "16945e89"
   },
   "source": [
    "## 8. Store the lexicon into excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f9bcc",
   "metadata": {
    "id": "a38f9bcc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workbook = Workbook(encoding = 'utf-8')\n",
    "table = workbook.add_sheet('data')\n",
    "table.write(0, 0, 'Content')\n",
    "table.write(0, 1, 'Meaning')\n",
    "table.write(0, 2, 'Polarity')\n",
    "table.write(0, 3, 'Sentiment')\n",
    "\n",
    "\n",
    "for i in range(0,len(flattened_cmplt_content)):\n",
    "    table.write(line, 0, flattened_cmplt_content[i].lower())\n",
    "    table.write(line, 1, flattened_cmplt_meaning[i])\n",
    "    table.write(line, 2, sentiment_score[i])\n",
    "    table.write(line, 3, sentiment_label(sentiment_score[i]))\n",
    "    line = line + 1\n",
    "        \n",
    "workbook.save('Malaysia Language.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M8ehHEQIhUTH",
   "metadata": {
    "id": "M8ehHEQIhUTH"
   },
   "source": [
    "## 9. Lexicon Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cAyzRrfbhUTH",
   "metadata": {
    "id": "cAyzRrfbhUTH"
   },
   "outputs": [],
   "source": [
    "def lexicon_mapping(token, lxc, n_grams, min_threshold):\n",
    "    \n",
    "    lexicon_matched = [] # Store the matched lexicons\n",
    "    scores_matched = [] # Store the sentiment scores of matched lexicons\n",
    "    no_matched_or_similar = [] # Store tokens that have no any matched or similar lexicon\n",
    "    words_to_rmv = [] # Store the two-grams tokens to remove from the original sentence if matched or similar lexicon found during two-grams mapping\n",
    "    mapped_single_w = [] # Store the one-gram tokens if matched or similar lexicon found during one-gram mapping\n",
    "    \n",
    "    # Map lexicon to get sentiment score\n",
    "    for t in token: # Iterate the cleaned token list\n",
    "        levenshtein_distance = [] # Store levenshtein distance if not matched\n",
    "        \n",
    "        for index, row in lxc.iterrows(): # Iterate the lexicon\n",
    "            if t == row['Content']: # Found matched lexicon\n",
    "                lexicon_matched.append(row['Content']) # Append the matched lexicon\n",
    "                scores_matched.append(row['Sentiment_Score']) # Append its sentiment score \n",
    "                if n_grams == 2:\n",
    "                    words_to_rmv.append(t)\n",
    "                elif n_grams == 1:\n",
    "                    mapped_single_w.append(t)\n",
    "                break\n",
    "            else: # Compute and append levenshtein distance if not matched with current lexicon\n",
    "                levenshtein_distance.append(enchant.utils.levenshtein(t, row['Content']))\n",
    "                \n",
    "        if len(levenshtein_distance) == len(lxc.index): # No match after scanning the entire lexicon\n",
    "            min_value = min(levenshtein_distance) # Get the lowest score (The lower, the similar)\n",
    "            \n",
    "            if min_value <= min_threshold: # Set a threshold where words with score lower or equal to the value are considered as similar\n",
    "                min_index = levenshtein_distance.index(min_value) # Get its index in the list\n",
    "                lexicon_matched.append(lxc.iloc[min_index][0]) # Get and append the matched lexicon\n",
    "                scores_matched.append(lxc.iloc[min_index][2]) # Get and append its sentiment score from the lexicon\n",
    "                if n_grams == 2:\n",
    "                    words_to_rmv.append(t)\n",
    "                elif n_grams == 1:\n",
    "                    mapped_single_w.append(t)\n",
    "            else: # No any similar lexicon found\n",
    "\n",
    "                if n_grams == 1:\n",
    "                \n",
    "                    # Translation\n",
    "                    tst = Translator()\n",
    "                    t = tst.translate(t)\n",
    "\n",
    "                    # Check if the current token is a valid English word after translation\n",
    "                    d1 = enchant.Dict(\"en_US\")\n",
    "                    d2 = enchant.Dict(\"en_GB\")\n",
    "                    if d1.check(t.text) or d2.check(t.text):\n",
    "\n",
    "                        # lemmatization\n",
    "                        lmtz = WordNetLemmatizer() \n",
    "                        t = lmtz.lemmatize(t.text) \n",
    "                        no_matched_or_similar.append(t)\n",
    "                \n",
    "    if n_grams == 1:\n",
    "        # Compute overall sentiment for tokens with no mapped lexicon\n",
    "        no_mapped_sentiment = sentiment_polarity(no_matched_or_similar) \n",
    "        return (lexicon_matched, scores_matched, no_mapped_sentiment, mapped_single_w)\n",
    "                \n",
    "    elif n_grams == 2:\n",
    "        return (lexicon_matched, scores_matched, words_to_rmv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zcBLTh5ihUTI",
   "metadata": {
    "id": "zcBLTh5ihUTI"
   },
   "source": [
    "## 10. Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_yir674jhUTI",
   "metadata": {
    "id": "_yir674jhUTI"
   },
   "outputs": [],
   "source": [
    "def overall_sentiment(r):\n",
    "    \n",
    "    return ( sum(r[1], r[2]) / (len(r[1]) + 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pUFEdjAhhUTJ",
   "metadata": {
    "id": "pUFEdjAhhUTJ"
   },
   "source": [
    "## 11. Two-grams Conversion and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "I7Ba5e3khUTJ",
   "metadata": {
    "id": "I7Ba5e3khUTJ"
   },
   "outputs": [],
   "source": [
    "def twoGramsMapping(inp, lxc):\n",
    "    \n",
    "    # Simple preprocessing\n",
    "    inp_l = inp.lower() # Convert to lower case\n",
    "    inp_c = inp_l.translate(str.maketrans(\"\", \"\", string.punctuation)) # Punctuation removal\n",
    "    \n",
    "    # Two-grams conversion\n",
    "    n = 2\n",
    "    two_grams = ngrams(inp_c.split(), n)\n",
    "    two_grams_t = []\n",
    "\n",
    "    for grams in two_grams:\n",
    "        two_grams_t.append(grams[0] + \" \" + grams[1])\n",
    "        \n",
    "    # Two-grams lexicon mapping\n",
    "    two_grams_mapping = lexicon_mapping(two_grams_t, lxc, 2, 2)\n",
    "    \n",
    "    if len(two_grams_mapping[2]) > 0:\n",
    "        for w in two_grams_mapping[2]:\n",
    "            inp_c = inp_c.replace(w, \"\")\n",
    "        \n",
    "    return (two_grams_mapping, inp_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TOa_T0LJvaW6",
   "metadata": {
    "id": "TOa_T0LJvaW6"
   },
   "source": [
    "## 12. Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "T0vU-d4Avftb",
   "metadata": {
    "id": "T0vU-d4Avftb"
   },
   "outputs": [],
   "source": [
    "def text_processing(t):\n",
    "    \n",
    "    # Read the Lexicon for Malaysia Rojak Language\n",
    "    lexicon = pd.read_excel(\"Malaysia Language.xls\")\n",
    "    \n",
    "    # Two-grams Mapping\n",
    "    twoGramsResult = twoGramsMapping(t, lexicon)\n",
    "    \n",
    "    # Text filtering\n",
    "    t_clean = textFilter(twoGramsResult[1], 0) \n",
    "    \n",
    "    # Perform one-gram lexicon mapping\n",
    "    mapping_results = lexicon_mapping(t_clean, lexicon, 1, 1)   \n",
    "    \n",
    "    # Combine results from one and two-grams mapping\n",
    "    if len(twoGramsResult[0][0]) > 0:\n",
    "        mapping_results = (mapping_results[0] + twoGramsResult[0][0], mapping_results[1] + twoGramsResult[0][1], mapping_results[2], mapping_results[3] + twoGramsResult[0][2])\n",
    "\n",
    "    # Compute overall sentiment\n",
    "    final_sentiment = overall_sentiment(mapping_results)\n",
    "    \n",
    "    return (mapping_results, final_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8MxmFB8Ct1F7",
   "metadata": {
    "id": "8MxmFB8Ct1F7"
   },
   "source": [
    "## 13. User input & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee61c57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ee61c57",
    "outputId": "2ce41ab0-498b-4882-e9e5-5848d1361c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Welcome to Rojakpedia ========================================\n",
      "\n",
      "Please enter your sentence or type -1 to exit >> bu yao bt bodoh sia leh\n",
      "########################################## Sentiment Results ##########################################\n",
      "#                                                                                                     #\n",
      "#   Attempt: 1                                                                                        #\n",
      "#   Your sentence: bu yao bt bodoh sia leh                                                            #\n",
      "#   Matched lexicons: bu=5u(0.0), yao=ya(0.0), sia=siam(0.25), leh=leh(0.1), bt bodoh=buat bodoh(-0...#\n",
      "#   Sentiment for unmatched words: 0.0000                                                             #\n",
      "#   Overall Sentiment: 0.0373                                                                         #\n",
      "#   Sentiment classification: positive                                                                #\n",
      "#                                                                                                     #\n",
      "#######################################################################################################\n",
      "\n",
      "Please enter your sentence or type -1 to exit >> u mau tun padang ke\n",
      "########################################## Sentiment Results ##########################################\n",
      "#                                                                                                     #\n",
      "#   Attempt: 2                                                                                        #\n",
      "#   Your sentence: u mau tun padang ke                                                                #\n",
      "#   Matched lexicons: ke=k(0.0), u mau=xmau(0.0), tun padang=turun padang(-0.1578)                    #\n",
      "#   Sentiment for unmatched words: 0.0000                                                             #\n",
      "#   Overall Sentiment: -0.0394                                                                        #\n",
      "#   Sentiment classification: negative                                                                #\n",
      "#                                                                                                     #\n",
      "#######################################################################################################\n",
      "\n",
      "Please enter your sentence or type -1 to exit >> fong fei kei\n",
      "########################################## Sentiment Results ##########################################\n",
      "#                                                                                                     #\n",
      "#   Attempt: 3                                                                                        #\n",
      "#   Your sentence: fong fei kei                                                                       #\n",
      "#   Matched lexicons: fei=dei(-0.05), kei=dei(-0.05)                                                  #\n",
      "#   Sentiment for unmatched words: 0.0000                                                             #\n",
      "#   Overall Sentiment: -0.0333                                                                        #\n",
      "#   Sentiment classification: negative                                                                #\n",
      "#                                                                                                     #\n",
      "#######################################################################################################\n",
      "\n",
      "Please enter your sentence or type -1 to exit >> -1\n"
     ]
    }
   ],
   "source": [
    "# Print title\n",
    "print(\"======================================== Welcome to Rojakpedia ========================================\\n\")\n",
    "\n",
    "# Set initial value\n",
    "attempt = 1\n",
    "\n",
    "# Accept user input\n",
    "txt = input(\"Please enter your sentence or type -1 to exit >> \")\n",
    "\n",
    "while(txt != \"-1\"): \n",
    "\n",
    "    # Process user input\n",
    "    results = text_processing(txt)\n",
    "    \n",
    "    # Matched lexicons output formatting\n",
    "    m_lxc = \"\"\n",
    "    for i in range(len(results[0][0])):\n",
    "        m_lxc = m_lxc + results[0][3][i] + \"=\" + results[0][0][i] + \"(\" + str(round(results[0][1][i], 4)) + \"), \"\n",
    "    m_lxc = m_lxc[:-2]\n",
    "    \n",
    "    # If no matched lexicon found\n",
    "    if m_lxc == \"\":\n",
    "        m_lxc = \"None\"\n",
    "    \n",
    "    # Sentiment results\n",
    "    print(\"########################################## Sentiment Results ##########################################\")\n",
    "    print(\"# %101s\" % \"#\")\n",
    "    print(\"#   Attempt: %-88d #\" % attempt)\n",
    "    if len(txt) > 79:\n",
    "        print(\"#   Your sentence: %-80s...#\" % txt[:80])\n",
    "    else:\n",
    "        print(\"#   Your sentence: %-82s #\" % txt)\n",
    "    if len(m_lxc) > 76:\n",
    "        print(\"#   Matched lexicons: %-77s...#\" % m_lxc[:77])\n",
    "    else:\n",
    "        print(\"#   Matched lexicons: %-79s #\" % m_lxc)\n",
    "    print(\"#   Sentiment for unmatched words: %-66.4f #\" % results[0][2])\n",
    "    print(\"#   Overall Sentiment: %-78.4f #\" % results[1])\n",
    "    print(\"#   Sentiment classification: %-71s #\" % sentiment_label(results[1]))\n",
    "    print(\"# %101s\" % \"#\")\n",
    "    print(\"#######################################################################################################\\n\")\n",
    "    \n",
    "    # Get next user input\n",
    "    txt = input(\"Please enter your sentence or type -1 to exit >> \")\n",
    "    \n",
    "    attempt = attempt + 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Rojakpedia_NLP_FINAL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
